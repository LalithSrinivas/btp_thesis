%\documentclass[PhD]{iitddiss}
%\documentclass[MS]{iitddiss}
% \documentclass[MTech]{iitddiss}
% \documentclass[Dual]{iitddiss}
%\documentclass[BTech]{iitddiss}
\documentclass[Other]{iitddiss}
% IF YOU USE THE OTHER OPTION, THEN YOU MUST FILL OUT THE PROGRAM OPTION BELOW TOO
\program{Bachelor of Technology}

% \usepackage{times}
 \usepackage{t1enc}

\usepackage{graphicx}
\usepackage{hyperref} % hyperlinks for references.
\usepackage{amsmath} % easier math formulae, align, subequations \ldots
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{natbib}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsfonts}
\restylefloat{table}
 \linespread{1.2}

\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}

\fancyhf{}

\rhead{\fancyplain{}{\thepage}} % predefined ()
\lhead{\fancyplain{}{\rightmark}} % 1. sectionname, 1.1 subsection name etc
\cfoot{\textcopyright \text{ } \the\year, \emph{Indian Institute of Technology Delhi}}
\renewcommand{\footrulewidth}{0.4pt}
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page

\title{REASONING OVER KNOWLEDGE BASES}

\author{Kaladi Lalith Satya Srinivas \\2017CS10340\\ Nagendra Kokku \\2017CS10343}
\advisor{Prof. Maya Ramanath}
\date{January 2021}
\department{Computer Science and Engineering}

%\nocite{*}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract

\abstract

\noindent KEYWORDS: \hspace*{0.5em} \parbox[t]{4.4in}{Reasoning over Knowledge Base; Sub-symbolic Reasoning; Neural Theorem Provers; Greedy Neural Theorem Provers; Conditional Theorem Provers.}

\paragraph{}
We study Neural Theorem Provers (NTPs), which are neural networks used to end-to-end differentiable proving of queries to the knowledge base by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. There are currently three iterations of Neural Theorem Provers. We want to study these and identify the bottlenecks and limitations and how well they perform on standard datasets larger than those used in the papers.

\vspace*{24pt}

\noindent 

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of contents etc.

\begin{singlespace}
\tableofcontents
\thispagestyle{empty}
\end{singlespace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Introduction
\chapter{THEORY}
\section{Introduction}
\paragraph{}
The prominent methods used for completing Knowledge Base (KB) are neural link prediction models that learn the distributed vector representations of symbols, allowing the model to encode similarities.  For example, If the vector of the predicate symbol grandfatherOf is similar to the vector of the symbol grandpaOf, then the model can conclude they express a similar relation. While this practice is great at encoding similarities, it often fails to capture more complex reasoning patterns that involve several inference steps. For example, if ABE is the father of HOMER and HOMER is a parent of BART, we would like to infer that ABE is the grandfather of BART. Such transitive reasoning is inherently hard for neural link prediction models as they only learn to score facts locally. In contrast, Symbolic theorem provers like Prolog are capable of this type of multi-hop reasoning but cannot learn representations of symbols and identify similarities between them.

\paragraph{}
Neural Theorem Provers (NTPs) are introduced as end-to-end differentiable provers for basic theorems formulated as queries. They use a backward chaining algorithm as a base to recursively build the neural network and learn the distributed vector representations of the symbols. The success score of the proof paths becomes differentiable with respect to vector representations of symbols and enables us to learn representations of entities, predicates, and other parameters involved in rule inducing. The later iterations of the model tried to make the model more efficient by limiting the choices of rules that need to be considered at any point in the proof path.

%Insert pseudo code


\section{Neural Theorem Prover}

\paragraph{}
In the following, we describe the recursive construction of NTPs – neural networks for end-to-end differentiable, proving that allow us to calculate the gradient of proof successes with respect to vector representations of symbols. NTPs can be divided into three main modules that are called upon recursively to prove a query. Each module takes an input of atoms and proof state and outputs a list of new proof states.
A proof state S = ( {$\psi$}, {$\rho$}) is a tuple, where {$\psi$} is the substitution set for the variable at that point of the proof, while {$\rho$} is the success score of the current proof.

\subsection{Unification module}
\paragraph{}
Unification of two atoms, i.e., a goal and a rule head, is the basis of backward chaining. In typical backward chaining used in Prolog, we try to match the symbols in the atoms, and if they don't, unification fails, and the proof can be aborted. In NTP, however, we look for similarity between the atoms and update the substitution set along with the success score, and output a new proof state. 

\paragraph{}
The signature of the unification module is L x L x S $\rightarrow$ S, where L is the list of terms or atoms (triples, usually) and S is a proof state. Unify iterates through both lists of terms, compares them, and updates them. If one of the symbols is variable, it is added to the substitution set. Otherwise, the vector representations of the symbols are used to calculate the similarity using Radial Base Function (RBF) kernel with a hyperparameter $\mu$.

\begin{enumerate}
	\item unify(H, G, S):
	\item match H, G with:
	\item \([], []\) $\rightarrow$ S
	\item \(\_, []\) $\rightarrow$ FAIL
	\item \([], \_\) $\rightarrow$ FAIL
	\item \(h: H^1, g: G^1 \rightarrow unify(H^1, G^1, S^1)\) where \(S^1 = (S_{\psi}^1, S_{\rho}^1)\)  and  \(S_{psi}^1 = \)
		\begin{enumerate}
			\item \(S_{\psi} \cup \{ h/g\}\) if \(h \in V\)
			\item \(S_{\psi} \cup \{ g/h\}\) if \(g \in V\) and \(h \notin V\)
			\item \(S_{\psi}\) otherwise
		\end{enumerate}
	\item and \(S_{\rho}^1 \) = 
		\begin{enumerate}
			\item min( \(S_{\rho}\), exp(\( - \lVert  {{\theta}_h - {\theta}_g}  \rVert / {2{\mu}^2} \)) ) if h, g $\notin$ V
			\item min(\(S_{\rho}\), 1) otherwise
		\end{enumerate}
\end{enumerate}

\subsection{OR module}
\paragraph{}
This model tries to apply rules to our goals and outputs the possible proof states. The signature of this module is L x $\mathbb{N}$ x S $\rightarrow$ $S^N$, where L is the set of atoms, $\mathbb{N}$ is the set of natural numbers which specify maximum depth of our proof and S is the set of proof states. We implement OR as:

\begin{enumerate}
	\item \( {or_{\theta}^{\mathbb{K}}}(G, d, S) = [S^1 |S^1 \in {and_{\theta}^{\mathbb{K}}(\mathbb{B}, d, {unify_{\theta}(H, G, S)})} for H:-\mathbb{B} \in \mathbb{K}]\)
	%[ S^\' \| S^\' \in {and_{\theta}^{\mathbb{K}}(\mathbb{B}, d, {unify_{\theta}(H, G, S)})} for H:-\mathbb{B} \in \mathbb{K}]
\end{enumerate}

where H :- $\mathbb{B}$ denotes every rule in KB $\mathbb{K}$, where H is the head atom of the rule and $\mathbb{B}$ denotes the list of atoms in the body. The or module recursively calls and module with the proof state obtained from the unification module.

\subsection{AND module}
\paragraph{}
The and module first uses a function substitute that substitutes all the variables in atoms according to the substitution set available. The substitue function is as follows:

\begin{enumerate}
	\item \( substitute([ ], \_) = [ ]\)
	\item \( substitute(g:G, \psi)\) = \{ $x$ if $g/x$ \(\in \psi, g\) otherwise\} \(: substitute(G, \psi)\)
\end{enumerate}

\paragraph{}
The signature of and module is L x $\mathbb{N}$ x S $\rightarrow$ $S^N$, where L is the domain of list of atoms, $\mathbb{N}$ is set of natural numbers which dictates the maximum depth of the proof and S is the set of proof states. The and module is as follows:

\begin{enumerate}
	\item \(and_{\theta}^{\mathbb{K}}(\_, \_, FAIL) = FAIL \)
	\item \(and_{\theta}^{\mathbb{K}}(\_, 0, \_) = FAIL \)
	\item \(and_{\theta}^{\mathbb{K}}([ ], \_, S) = S \)
	\item \(and_{\theta}^{\mathbb{K}}(G:\mathbb{G}, d, S) = [ S^2 | S^2 \in {and_{\theta}^{\mathbb{K}}(\mathbb{G}, d, S^1)}\) for \(S^1 \in {or_{\theta}^{\mathbb{K}}(substitute(G, S_{\psi}), d-1, S)} ] \)
\end{enumerate}

\subsection{Bottleneck}
\paragraph{}
We can observe that in the or module, we iterate through every rule and fact present in our KB $\mathbb{K}$ at every stage of our proof. This is a highly inefficient way of expanding our proofs since we only use the path which gives the maximum score for updating our model parameters. This is especially infeasible on large knowledge bases, not only in terms of time but also memory, as we have to store all these proof states in memory and pick the maximum score. A serious optimization on reducing the scope of rules to be expanded upon is required.

\begin{figure}[H]
  \begin{center}
  	\captionsetup{format=plain}
    \resizebox{170mm}{!} {\includegraphics *{ntp_example.png}}
    \caption {Exemplary construction of an NTP computation graph for a toy knowledge base. Indices
on arrows correspond to application of the respective KB rule. Proof states (blue) are subscripted
with the sequence of indices of the rules that were applied. Underlined proof states are aggregated to
obtain the final proof success. Boxes visualize instantiations of modules (omitted for unify). The
proofs S33, S313 and S323 fail due to cycle-detection (the same rule cannot be applied twice).}
  \label{fig:time-batch}
  \end{center}
\end{figure}
	

\section{Greedy Neural Theorem Prover}
\paragraph{}
Greedy NTPs (GNTPs) tries to address the complexity and scalability of NTPs. As mentioned in the previous section, the bottleneck of NTP is that it iterates through the rules and facts present in the Knowledge Base (KB). We also mentioned that all the proof states that are not the maximum would be discarded at the end; thus, selecting the rules that produce the best scores is important by some heuristic. A couple of heuristics are introduced for selecting facts (at the terminal stage, depth=1) and for selecting rules based on the goal.

\subsection{Fact selection}
\paragraph{}
Unifying subgoal with all facts in our KB is not feasible for larger KBs. Considering the expression, \[ ntp_{\theta}^{\mathbb{K}}(G, 1) = max_{F:- [ ] \in \mathbb{K}} S_{\rho}^{F}\]

where $S_{\rho}^{F}$ is the unification score of G with the fact F. NTP implementation currently iterates through all the facts and take the maximum score as the output. The order of complexity of this approach is \( O(|\mathbb{K}|n)\), where n is the number of subgoals. This means that, at inference time, we only need the largest proof score for returning the correct output. Similarly, during training, the gradient of the proof score with respect to the parameters $\theta$ can also be calculated exactly by using the single largest proof score:

\[ \frac { \partial ntp_{\theta}^{\mathbb{K}}(G, 1)_{\rho}}{\partial \theta} = \frac {\partial max_{F:- [ ] \in \mathbb{K}} S_{\rho}^{F}}{\partial \theta} = \frac {\partial S_{\rho}^{*}}{\partial \theta} \]


\paragraph{}
GNTP uses the Nearest Neighbour Search (NNS) to efficiently compute the maximum score $S^*$. It uses the exact and approximate NNS framework for efficiently searching $\mathbb{K}$ for the best supporting facts for a given sub-goal. Specifically, it uses the exact L2-nearest neighbor search, and, for the sake of efficiency, we update the search index every ten batches, assuming that the small updates made by stochastic gradient descent do not necessarily invalidate previous search indexes.

\subsection{Rule selection}
\paragraph{}
A similar idea is used to select rules for proving a given goal G. We expand the proofs with rules whose rule heads are closest to the goals. We can partition the rules and facts that are present in our KB $\mathbb{K}$ that has the same template. We can select the best k rules that are of certain template which may produce the best scores. The or module is adapted as follows:

\[ or_{\theta}^{\mathbb{K}}(G, d, S) = [ S^1 | H :- \mathbb{B} \in N_{P}(G), P \in \beta, S^1 \in and_{\theta}^{\mathbb{K}}(\mathbb{B}, d, unify_{\theta}(H, G, S)) ] \]

where P is the template of the rules to pick from, and H :- $\mathbb{B}$ is the best rules in the neighborhood of goal G under the template P ($N_P$). So instead of unifying the goal with all rule heads, it constrains the unification to only the rule heads in the neighborhood of G.

\subsection{Bottleneck}
The bottleneck in the case of GNTP is updating the search indexes used for the NNS framework every few batches. This can even be observed in the substantial growth of run time for the batch in which an update of the search index is done.


\section{Conditional Theorem Prover}
\paragraph{}
Similar to GNTP, CTP tries to efficiently select rules upon which to expand the proof. CTP does it by introducing a new module select, which is essentially a neural network that takes the vector representation of the goal head as an input and outputs the rule with a defined template. The parameters of the select module are trainable, and the module will learn rules that agree with the data present in our Knowledge Base (KB).

\paragraph{}
So the or module will be modified to accommodate the select module instead of iterating through all rules present in our KB. The or module is as follows:

\( or(G, d, S) = [ S^1 | \) for \( H:-\mathbb{B} \in select_{\theta}(G) and S^1 \in and(\mathbb{B}, d, unify(H, G, S)) ] \)

Several variations of the select module are also introduced, which include linear, attentive, and memory-based goal reformulators.

\subsection{Goal Reformulators}
\subsubsection{Linear Reformulator}
\paragraph{}
Here, select module is defined as a linear function of the goal predicate. For example, for a template of 1 head, 2 body terms rule, it will be as follows:

\[ select_{\theta}(G) = [ F_H(G) :- F_{B_1}(G), F_{B_2}(G)] \]

where
\begin{enumerate}
	\item \( F_H(G) = [ f_H(\theta_G), X, Y] \)
	\item \( F_{B_1} = [ f_{B_1}(\theta_G), X, Z] \)
	\item \( F_{B_2} = [ f_{B_2}(\theta_G, Z, Y] \)
\end{enumerate}

where each $f_i: \mathbb{R}^k \rightarrow \mathbb{R}^k$ is a linear function of the form $f_i(x) = W_{i}x+b_{i}$, where $W_i \in \mathbb{R}^{k*k} $ and $ b_i \in \mathbb{R}^k$. Thus similar to GNTP, instead of iterating through all the rules in KB, we can apply this linear transformation on our goal predicate to get the rule we need to expand upon.

\subsubsection{Attentive Reformulator}
\paragraph{}
This reformulator includes a useful prior to the select module, namely the predicate symbols that are present in our KB. The method used to incorporate this prior is to generate distributions of goal G over all the predicates present in our KB. Let the set of predicates/relations be R. The select module is as follows:

\[ f_i(x) = \alpha E_R\]
\[\alpha = softmax(W_{i}x) \in \Delta^ {(|R|-1)}\]

where $E_R \in \mathbb{R}^{|R|*k}$ is the predicate embedding matrix of R, $W_i \in \mathbb{R}^{k*|R|}$ and $\alpha \in \Delta^{|R|-1}$ is the attention distribution over the predicates R. This method is especially useful if the number of predicates is less than embedding size and also provides meaningful predicates in rules faster.


\subsubsection{Memory-Based Reformulator}
Memory-based reformulators help us to inspect what kind of rules are being formed by the select module. We store the n rules in our KB in memory matrices $[M_1, M_2, ... M_m]$, where $M_i \in \mathbb{R}^{n*k}$ contains the i-th predicate of all the n rules. Now similar to attentive reformulator, we compute an attentive distribution over these n rules using our goal predicate embedding. The rule is obtained by taking the weighted average of the rules in the memory matrices. We can also inspect the rules better by observing the memory matrices and attention distribution for the predicates.

\[ f_i(x) = \alpha M_i\]
\[ \alpha = softmax(Wx) \in \Delta^{n-1}\]

where $f_i: \mathbb{R}^k \rightarrow \mathbb{R}^k$ is a differentiable function that gives the goal produces an attention distribution $\alpha \in \Delta^{n-1}$ over the rules present in our memory.

\subsection{Bottleneck}
\paragraph{}
The bottleneck with respect to time and memory in this model comes from the step where unification scores are calculated with every fact present in our Knowledge Base (KB) at the terminal node step. This effect is particularly visible in larger datasets. A similar approach of GNTP can be used for selecting facts while still adopting the select structure to generate rules can be used to overcome this bottleneck, though we still need to pursue this idea. 



% Experiments.


\chapter{EXPERIMENTS}
\section{Introduction}
\subsection{Setup}
\paragraph{}
We have conducted all our experiments on Baadal (IITD’s cloud computing platform) Virtual Machine with 32GB Memory. We have used Tensorflow version 1.12.0 for executing GNTP code and PyTorch for executing CTP code. We have made changes to both the models so that they use python pandas to read triples from data files. This makes the file reading part faster.

\paragraph{}
Following are the steps followed to run the models on the machine.

\begin{enumerate}
	\item Install conda and set up a virtual environment with TensorFlow version 1.12 and python version as 3.6.
	\item Run the command \verb|pip install -r requirements.txt| in folders containing these modules, separately. This will install all dependency libraries.
	\item After installing all the dependencies, run the command \verb|python setup.py install| in both the folders to install the models.
	\item Finally, enter the command to run the models. Example commands for CTP and GNTP are mentioned below.
	\item For executing CTP: 
		\begin{verbatim}
			python3 ./bin/gntp-moe-cli.py
			    --train data/wn18rr/train.tsv 
			    --dev data/wn18rr/dev.tsv
			    --test data/wn18rr/test.tsv
			    -c data/wn18rr/clauses.v1.pl -E ranking --max-depth 1 -b 1000 --corrupted-pairs 1
			    -l 0.005 --l2 0.001 --k-max 10 --all -F 5 -R 1 -I 100 --seed 0 --model-type ntp
			    --initializer uniform --rule-type standard --decode --kernel rbf --unification-type joint
			    -i faiss -e 100 --auxiliary-epochs 0 --test-batch-size 10000
			    --only-rules-epochs 95 --only-rules-entities-epochs 0 --input-type standard
			    --train-slope --check-path data/wn18rr/dev.256.tsv --check-interval 1000
		\end{verbatim}
		\newpage
	\item For executing GNTP:
		\begin{verbatim}
			python3 ./bin/clutrr-cli.py 
				--train data/clutrr-emnlp/data_db9b8f04/1.2,1.3,1.4_train.csv 
				--test data/clutrr-emnlp/data_db9b8f04/1.10_test.csv  
				-S 1 -s concat -V 128 -b 32 -d 2 --test-max-depth 4 --hops 2 2 2 2 -e 5 
				-o adagrad -l 0.1 --init random --ref-init random 
				-m 5 -t min -k 20 -i 1.0 -r memory -R 256 --seed 1
		\end{verbatim}
\end{enumerate}

\subsection{Datasets}
\paragraph{} 
The Authors have included a few standard datasets like WN18RR, WN18, FB15K-237, Nations, Kinship, UMLS, etc. they have also used these datasets to compare GNTP and CTP with other link-prediction models like ComplEx, TransE, etc. The datasets and the number of facts in each dataset are mentioned below. 

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textit{\textbf{Name of Dataset}} & \textbf{Number of Facts} \\ \hline
		Countries                               & 1,158                  \\ \hline
		Nations                        & 2,565                   \\ \hline
		Kinship                        & 10,686                   \\ \hline
		UMLS                       & 6,529                  \\ \hline
	\end{tabular}
	\caption{Table showing Datasets and Number of Facts.}
\label{tab:my-table}
\end{table}

%% Have to write about CLUTTR
\paragraph{}
The Authors also used a dataset called CLUTTR, which is significantly larger than the ones mentioned above, to showcase that CTP can handle larger data better compared to GNTP. But when we looked at CLUTTR dataset, we observed that this is a special dataset in which the bottleneck of CTP goes away. CLUTTR contains rows in which there is a story, which is the collection of local facts for that row, and it also has some queries, which are the goal atoms we will work with. So for every goal at a terminal node, we only need to unify it with the facts from the same row, which is a significantly smaller set of facts than the complete facts in KB, like in other cases. Thus we think that CLUTTR is not reliable dataset to make the conclusion that CTP handles bigger data better than GNTP, which we also reflected later in the experiments we conducted, where GNTP outperforms CTP in terms of memory usage and time taken.

\paragraph{} 
It can be observed from the table above that these datasets are far smaller when compared to real-world knowledge bases. Hence, we have collected larger datasets that are within the models limits. One of the main datasets that we have considered is YAGO. The primary YAGO dataset contains around 2 billion facts. As both CTP and GNTP store all the facts in the memory, it will not be feasible to run these models on this dataset given a machine with 32GB memory. So we have considered a liter version of YAGO, which has 5.4 Million facts. We tried running both the model with the same parameters used by the Authors. The memory got exhausted. Hence we tried fine-tuning the hyper-parameters. Though we were able to execute GNTP, after changing hyper-parameters like batch size, which has consumed ~29GB of memory, we could not run CTP. Hence, we created datasets by sampling entities from the lighter-YAGO dataset to run experiments to observe Memory utilization and execution time limitations. We have created datasets by sampling top-k frequent, least-k frequent, random-k entities and extracted all the facts from the dataset related to these entities to conduct our experiments.
\\

\begin{table}[]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textit{\textbf{Number of Entities}} & \textbf{Number of Facts} \\ \hline
		Top 10                               & 679,326                  \\ \hline
		Random 10,000                        & 33,793                   \\ \hline
		Random 10,000                        & 22,774                   \\ \hline
		Random 100,000                       & 243,830                  \\ \hline
		Random 100,000                       & 329,850                  \\ \hline
		Random 270,000                       & 612,996                  \\ \hline
		Random 270,000                       & 634,349                  \\ \hline
		Bottom 700,000                       & 595,601                  \\ \hline
	\end{tabular}
	\caption{Table showing Number of Entities and Number of Facts in each Datasets sampled.}
\label{tab:datasets}
\end{table}

\subsection{Versioning, Dependency Issues}
\paragraph{} 
Firstly, we tried replicating the results that the authors have claimed. We ran into a few versioning issues (for GNTP) like the version of TensorFlow, dependency libraries, python version, etc. We have tried the versions mentioned in requirements.txt and setup.py, but we were not able to run the module. So, we manually checked for the version that satisfies all conditions and changed requirements.txt and setup.py accordingly.

\subsection{Sampling Dataset}
\paragraph{} 
As mentioned before, we tried using the lighter version of YAGO. We sampled the data for the following reasons.

\begin{enumerate}
	\item To observe how memory utilization, execution time depends on the Number of Entities and the Number of Facts.
	\item Compare GNTP and CTP in terms of Memory utilization and execution time.
	\item Maximum size of KB, each model, can handle, given a 32 GB machine.
	\item Accuracy improvement per epoch for each model.
\end{enumerate}

Hence we have made a script to sample the data. The script first goes through all the triples and store the frequency of an entry as a subject or object, predicate. After this, we have sampled top-10 frequent entities (an entity is one that was present as a subject or object). Analyzing top frequent entities provides us a situation with less number of entities and more number of facts. Similarly, analyzing the least frequent entities helps us compare them with a similar number of facts and a far more number of entities. We have randomly sampled 10K, 100K, 270K entities and extracted all the facts related to these entities. We have used these datasets as our base datasets for analyzing and observing limitations in terms of memory utilization and execution time. We have also used standard datasets by FreeBase and WordNet for analyzing loss, accuracies of the models, which are still work in progress at the time.

\section{Replicating the Results}
\paragraph{}
After resolving all versioning and dependency issues, we tried executing the modules on the standard datasets to replicate the author's results. By default, the GNTP model evaluates/validates the model for every 1000 batches. The estimated evaluation time was around 26 hours. There were 8684 batches, so it took 10-11 days for the GNTP model to execute completely. The results after complete execution were in accordance with the results mentioned in the paper with a slight margin of error. 

\paragraph{}
Similarly, we have executed CTP on a CLUTTR dataset. The size of the dataset file is larger than that of GNTP's, but the execution time is many folds less. We have gone through the CLUTTR dataset and CTP code to reason why CTP is much faster and accurate though the dataset is larger than that of GNTP.

\paragraph{Metrics:}
Mean Reciprocal Rank (MRR), Hits@m for m = 1, 3, 5, 10.

\subsection{Analysis}
\paragraph{}
The CTP essentially have to go through all the facts at the terminal node to compute scores and select the facts with the best scores. As mentioned before, each CLUTTR data entry has a story associated with it. Because of this property, we don't have to go through all the facts, but just the ones associated with the story. Hence, it makes the bottleneck of CTP disappear as the number of facts to iterate at the terminal node decreases drastically. This is not true in the case of general datasets involving triples. The authors have used CLUTTR dataset to show that CTP can handle larger datasets when compared to GNTP. But this claim is only true under certain conditions, like the dataset should be of CLUTTR format. Hence we have started to analyze the performance of CTP and GNTP on more general datasets.

\section{Memory Utilization Analysis}

\paragraph{}
As the author claims that CTP can handle larger datasets better than GNTP, we tried comparing memory utilized by GNTP and CTP. We also wanted to observe how memory utilized by these models is related to a number of Entities and a number of Facts.

\paragraph{Datasets:}
We have used the datasets mentioned in the Table \ref{tab:datasets} for this experiment.

\paragraph{Hyper-parametres:}
The following are the hyper-parameters used while running both models.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Hyper-parameter Name} & \textbf{Value} \\ \hline
		Embedding Size                & 20             \\ \hline
		Learning Rate                 & 0.1            \\ \hline
		Batch Size                    & 32             \\ \hline
		Reformulator (for CTP)        & Linear         \\ \hline
	\end{tabular}
	\caption{Table Showing hyper-parameters and their values}
	\label{tab:hyperparameters}
\end{table}

\paragraph{Method: }
We have executed the command to run each module (separately) on a dataset and observed how the net memory varies for every batch. We have repeated this process on all the datasets we have mentioned.

\paragraph{Metrics:}
On executing the command \verb|free -mh|, we can view the free and used memory in the machine. The 'used' memory is the metric we have chosen to estimate the memory used by each model.

\subsection{Results}
\paragraph{}

\begin{longtable}[c]{|l|l|l|l|}
		\hline
		\textbf{Entities} & \textbf{Facts} & \textbf{Memory (by GNTP)} & \textbf{Memory (by CTP)} \\ \hline
		Top 10            & 679326         & 1.9 GB            & 4.9 GB           \\ \hline
		Random 10,000     & 33,793         & 542 MB           & 786 MB          \\ \hline
		Random 10,000     & 22,774         & 498 MB           & 699 MB          \\ \hline
		\endfirsthead
%
\multicolumn{4}{c}%
{{\bfseries Table \thetable\ continued from previous page}} \\
\endhead
%
		Random 100,000    & 243,830        & 1.6 GB           & 2.2 GB          \\ \hline
		
		Random 100,000    & 329,850        & 2.0 GB           & 2.8 GB          \\ \hline
		Random 270,000    & 612,996        & 3.3 GB           & 4.8 GB          \\ \hline
		Random 270,000    & 634,349        & 3 GB              & 4.7 GB          \\ \hline
		Bottom 700,000    & 595,601        & 4 GB              & 4.8 GB          \\ \hline
	\caption{Table Showing memory utilized by each model when run on the corresponding dataset.}
	\label{tab:memory-utilization}
\end{longtable}

\subsection{Analysis}
\paragraph{}
It can be observed from Figure \ref{fig:memory} that, as both numbers of facts and number of entities increases, till 700K entities, the memory utilized by both the models increases. But after that, as mentioned before, the number of entities decreases, and the number of facts increases. It can be observed that the memory utilized by GNTP decreases, whereas memory utilized by CTP increased slightly.

\begin{figure}[H]
  \begin{center}
    \resizebox{150mm}{!} {\includegraphics *{memory.png}}
    \caption {Plot showing Number of Facts on X-axis, Memory (in MBs) on left vertical axis and number of Entities on right vertical axis}
  \label{fig:memory}
  \end{center}
\end{figure}

\paragraph{}
The memory utilized by CTP is always greater than that of GNTP. But, considering the almost similar number of facts, the difference between GNTP and CTP increases as the number of Entities decreases. 

\paragraph{}
We can say that the memory utilized by GNTP is more sensitive to a number of entities that to a number of facts. In the case of CTP, memory utilized by this model is more sensitive to a number of facts than to a number of entities. 

\section{Execution Time Analysis}

\paragraph{}
As mentioned before, GNTP took a lot of time to execute a small dataset, but CTP took far less time when run on the CLUTTR dataset. Our goal is to analyze how these models perform on larger datasets, with triples, in terms of time. This gives us a better idea of which model performs better while handling larger datasets in terms of execution time.

\paragraph{Datasets:}
We have used the datasets mentioned in the Table \ref{tab:datasets} for this experiment.

\paragraph{Hyper-parametres:}
The hyper-parameters used for this experiment are the same as the parameters mentioned in Table \ref{tab:hyperparameters}.

\paragraph{Method: }
To observe the time it takes for each model to train a batch, we have made few changes to the source code of both models. After re-installing the models, we executed them independently, one at a time, and observed the average time it takes to train one batch for both the models. We have extrapolated this average batch times to obtain epoch time too.

\paragraph{Metrics:}
The average time it takes for a model to train a batch, time it takes for a model to complete one epoch.

\subsection{Results}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|r|r|}
		\hline
		\textbf{Correlation} & \multicolumn{1}{l|}{\textbf{CTP}} & \multicolumn{1}{l|}{GNTP} \\ \hline
		Entities Vs Memory   & 0.5891616523                      & 0.8617175819              \\ \hline
		Facts Vs Memory      & 0.9959181818                      & 0.8900775666              \\ \hline
		Entities Vs Time     & 0.07485990126                     & 0.160307427               \\ \hline
		Facts Vs Time        & 0.9923655232                      & 0.9848214856              \\ \hline
		
	\end{tabular}
	\caption{Table Showing correlation between Number of Entities, Facts and Memory, Time}
	\label{tab:correlation}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{Entities} & \textbf{Facts} & Time per batch (by GNTP) & Time per batch (by CTP) \\ \hline
		Top 10            & 679326         & 3-5 sec                  & 480 sec                   \\ \hline
		Random 10,000     & 33,793         & 0.2 sec                  & 13 sec                  \\ \hline
		Random 10,000     & 22,774         & 0.15 sec                 & 8.3 sec                 \\ \hline
		Random 100,000    & 243,830        & 1.5 sec                     & 130 sec            \\ \hline
		Random 100,000    & 329,850        & 1.7 sec                  & 160 sec            \\ \hline
		Random 270,000    & 612,996        & 3.3 sec                  & 420 sec                   \\ \hline
		Random 270,000    & 634,349        & 3.4 sec                  & 450 sec                 \\ \hline
		Bottom 700,000    & 595,601        & 3-3.5 sec                  & 420 sec                   \\ \hline
	\end{tabular}
	\caption{Table Showing average time (in seconds) it takes to train a batch by each model when run on the corresponding dataset.}
	\label{tab:execution-time}
\end{table}


\subsection{Analysis}
\paragraph{}
The relation between Time per batch/epoch and number of Facts/Entities is not so clear from the plots \ref{fig:time-batch}, \ref{fig:time-epoch}. Hence we have observed the correlation between time and number of facts, entities. From the table \ref{tab:correlation} it can be observed that
there is almost zero correlation between time and number of entities. Whereas the correlation between time and number of facts is almost 1.
\begin{figure}[H]
  \begin{center}
    \resizebox{150mm}{!} {\includegraphics *{time_batch.png}}
    \caption {Plot with Number of Facts on X-axis, Time per Batch (in sec, log-scale) on left vertical axis and number of Entities on right vertical axis}
  \label{fig:time-batch}
  \end{center}
\end{figure}

\paragraph{}
Hence, we can say that the execution time is almost independent of number of entities whereas fully dependent and proportional to a number of facts. It can also be observed from the table \ref{tab:execution-time} and plot \ref{fig:time-batch} that execution time of GNTP is less than CTP in many folds. 

\begin{figure}[H]
  \begin{center}
    \resizebox{150mm}{!} {\includegraphics *{time_epoch.png}}
    \caption {Plot with Number of Facts on X-axis, Time per Epoch (in sec, log-scale) on left vertical axis and number of Entities on right vertical axis}
  \label{fig:time-epoch}
  \end{center}
\end{figure}

\section{Performance Analysis}

\paragraph{}
GNTP came out to be superior in terms of execution time and memory utilization over CTP. To analyze performance efficiency, we have to compare the final scores and accuracies of these models. But, it takes so much time to run entirely, even on smaller datasets, so we cannot compare the accuracy as of yet. Hence we have compared the Loss per epoch of both models.

\paragraph{Datasets:}
To analyze performance, the datasets we have created (Table \ref{tab:datasets}) cannot be used as these datasets are not complete and standard. Hence, we have used a standard dataset of FreeBase.

\paragraph{Hyper-parametres:}
The hyper-parameters used for this experiment are the same as the parameters mentioned in Table \ref{tab:hyperparameters}, except the batch size, which we've changed to 1000.

\paragraph{Method: }
CTP and GNTP both use Binary Cross Entropy as their scoring function, but GNTP sums up individual scores, but CTP uses the mean of all individual scores. Hence, we have changed the source code of GNTP to make 'mean' the default option. Then, we have parsed the log files of both models to tabulate loss per epoch. In the case of GNTP, by default, only loss per batch is displayed. Hence we used the mean and standard deviation of these losses for every epoch (i.e., total triples/ triples per batch = number of batches per epoch) as Loss and error-in-loss of GNTP.

\paragraph{Metrics:}
Loss, error-in-loss after every epoch.

\subsection{Results}
\paragraph{}
Following are the results that we have observed.

\begin{longtable}[c]{|r|r|r|r|r|}
		\hline
		\multicolumn{1}{|l|}{\textbf{Epoch}} & \multicolumn{1}{l|}{\textbf{Loss (CTP)}} & \multicolumn{1}{l|}{\textbf{Error (CTP)}} & \multicolumn{1}{l|}{\textbf{Loss (GNTP)}} & \multicolumn{1}{l|}{\textbf{Error (GNTP)}} \\ \hline
		1                                    & 1.54                                     & 0.04                                      & 40.81                                     & 5.14                                       \\ \hline
		2                                    & 1.4                                      & 0.04                                      & 25.98                                     & 3.47                                       \\ \hline
		3                                    & 1.28                                     & 0.03                                      & 16.22                                     & 2.23                                       \\ \hline
		4                                    & 1.18                                     & 0.03                                      & 10.01                                     & 1.4                                        \\ \hline
		\endfirsthead
%
\multicolumn{5}{c}%
{{\bfseries Table \thetable\ continued from previous page}} \\
\endhead
%
		5                                    & 1.09                                     & 0.02                                      & 6.18                                      & 0.85                                       \\ \hline
		6                                    & 1.01                                     & 0.02                                      & 3.91                                      & 0.49                                       \\ \hline
		7                                    & 0.94                                     & 0.02                                      & 2.58                                      & 0.29                                       \\ \hline
		8                                    & 0.89                                     & 0.02                                      & 1.95                                      & 0.1                                        \\ \hline
		9                                    & 0.84                                     & 0.01                                      & 1.62                                      & 0.07                                       \\ \hline
		10                                   & 0.79                                     & 0.01                                      & 1.37                                      & 0.08                                       \\ \hline
		11                                   & 0.76                                     & 0.01                                      & 1.2                                       & 0.19                                       \\ \hline
		12                                   & 0.73                                     & 0.01                                      & 1.33                                      & 0.13                                       \\ \hline
		13                                   & 0.7                                      & 0.01                                      & 0.98                                      & 0.09                                       \\ \hline
		14                                   & 0.67                                     & 0.01                                      & 0.75                                      & 0.04                                       \\ \hline
		15                                   & 0.65                                     & 0.01                                      & 1.22                                      & 0.49                                       \\ \hline
	\caption{Table Showing Loss, Error-in-loss per epoch of CTP and GNTP. }
	\label{tab:loss}
\end{longtable}

\subsection{Analysis}

\paragraph{}
We can observe from the plot \ref{fig:loss} loss after the first epoch for CTP is far less than GNTP. Though the loss of GNTP reduces rapidly compared to CTP, CTP will produce better results after very few epochs while GNTP takes more epochs to achieve similar results. CTP can reach a sub-optimal position after a very few epochs compared to GNTP. This observation can be used to make a model that is space, time-efficient as GNTP, and performance efficient as CTP. A thorough analysis is pending in this particular aspect.

\begin{figure}[H]
  \begin{center}
    \resizebox{150mm}{!} {\includegraphics *{loss.png}}
    \caption {Plot with Number of Epochs on X-axis, loss per epoch of both the models on Y-axis.}
  \label{fig:loss}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography.

\begin{singlespace}
  \bibliography{refs}
\end{singlespace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% List of papers

\chapter{References}

\begin{enumerate}
\item Minervini, Pasquale, et al. "Learning reasoning strategies in end-to-end differentiable proving." International Conference on Machine Learning. PMLR, 2020.
\item Rocktäschel, Tim, and Sebastian Riedel. "End-to-end differentiable proving." Advances in Neural Information Processing Systems. 2017.
\item Minervini, Pasquale, et al. "Differentiable Reasoning on Large Knowledge Bases and Natural Language." (2020): 125-142.
\item Trouillon, Théo, et al. "Complex embeddings for simple link prediction." International Conference on Machine Learning (ICML), 2016.
\item Bordes, Antoine, et al. "Translating embeddings for modeling multi-relational data." Advances in neural information processing systems 26 (2013): 2787-2795.
\item Source code of CTP: \url{https://github.com/uclnlp/ctp}
\item Source code of GNTP: \url{https://github.com/uclnlp/gntp}
\end{enumerate}

\end{document}

